{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Solution to the Towers of Hanoi Puzzle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_David Edwards_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In which I learn the benefits of better reading the requirements thoroughly...\n",
    "\n",
    "Due to my being a single parent last week, I went through and did all of the printing, valid Moves, making and unmaking moves (which I ended up not using) and doing the myTupler, which takes a state and makes it into a tupler.\n",
    "\n",
    "Point 1 of reading the requirements, whereas I needed to type `Q[(myTupler(state), move)]` if I had implemented the `stateMoveTuple(state, move)` as mentioned in the assignment, I could have used the more readable `Q[stateMoveTuple(state,move)]`\n",
    "\n",
    "My next issue was using 1 (positive) for the goal state.  My original Q update line in `trainQ` looked like this:\n",
    "\n",
    "`Q[(myTupler(stateOld), moveOld)] += rho * (Q[(myTupler(state), move)] - Q[(myTupler(stateOld), moveOld)])`\n",
    "\n",
    "It actually kinda worked, and populated Q, but incorrectly.  The values all tended to be low numbers, trending towards zero.  After re-reading and seeing the -1, that was an easy change to make, but it broke everything.  After reading the Piazza post on what a proper Q dictionary would look like, I was able to tinker with that line until I realized I needed to include that -1 in the calculation.  A few more iterations, and I came up with this:\n",
    "\n",
    "`Q[(myTupler(stateOld), moveOld)] += rho * (-1+Q[(myTupler(state), move)] - Q[(myTupler(stateOld), moveOld)])`\n",
    "\n",
    "After that, everything just worked perfectly.\n",
    "\n",
    "Once I had Q properly created, the testQ function was almost trivial.  I say _almost_ because I couldn't figure out where in Q we should start.  I spent an unfortunate amount of time thinking about that, before I realized that the start state should be the same as when training Q.  Once I figured that out, all I had to do was lookup the current state in Q, determine which was the largest value (closest to 0), and make that move, appending it to the list to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy as copy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def printState(state):\n",
    "    '''\n",
    "    Prints a Tower of Hanoi state.  Now, with added pegs.\n",
    "    :param state: list of lists representing tower of hanoi state\n",
    "    :return: prints out the state nicely.\n",
    "    '''\n",
    "    stateCopy = copy.deepcopy(state)\n",
    "    maxLength = findLongest(stateCopy)\n",
    "    for item in stateCopy:\n",
    "        for x in range (len(item) , maxLength):\n",
    "            item.insert(0, ' ')\n",
    "\n",
    "    for i in range(maxLength):\n",
    "        for j in range(len(stateCopy)):\n",
    "            print(\"  {}\".format(stateCopy[j][i]), end='')\n",
    "        print()\n",
    "    print('--|--|--|--\\n\\n')\n",
    "\n",
    "\n",
    "def findLongest(state):\n",
    "    '''\n",
    "    Finds the longest (highest) peg in Towers of Hanoi.  Used to display in printState\n",
    "    :param state: list of lists representing tower of hanoi state\n",
    "    :return: length of longest item\n",
    "    '''\n",
    "    length = 0\n",
    "    for item in state:\n",
    "        length = max(length, len(item))\n",
    "\n",
    "    return length\n",
    "\n",
    "\n",
    "def validMoves(state):\n",
    "    '''\n",
    "    Returns a list of lists representing valid tower of hanoi moves from the given state.\n",
    "    :param state: list of lists representing tower of hanoi state\n",
    "    :return: list of lists representing valid tower of hanoi moves from the given state.\n",
    "    '''\n",
    "    validStates = []\n",
    "\n",
    "    for itemToMove in state:\n",
    "        # Check if the list is empty\n",
    "        if itemToMove:\n",
    "            # Iterate through the rest of the states\n",
    "            for placeToMove in range(len(state)):\n",
    "                # don't count as valid move if it's back to the same location\n",
    "                if state.index(itemToMove) != placeToMove:\n",
    "                    # valid move if the place to move is empty or the item to move is smaller\n",
    "                    if len(state[placeToMove]) == 0 or itemToMove[0] < state[placeToMove][0]:\n",
    "                        # append the tuple (move from, move to) to the valid states\n",
    "                        validStates.append([state.index(itemToMove)+1, placeToMove+1])\n",
    "    return validStates\n",
    "\n",
    "\n",
    "def makeMove(state, move):\n",
    "    '''\n",
    "    Takes a move and makes it on a tower of hanoi state\n",
    "    :param state: list of lists representing tower of hanoi state\n",
    "    :param move: tuple representing a move from (peg1,peg2)\n",
    "    :return:the state after the move was made\n",
    "    '''\n",
    "    item = state[move[0]-1].pop(0)\n",
    "    state[move[1]-1].insert(0, item)\n",
    "    return state\n",
    "\n",
    "\n",
    "def unMakeMove(state, move):\n",
    "    '''\n",
    "    Reverses a move made by makeMove.  No longer used.\n",
    "    :param state: list of lists representing tower of hanoi state\n",
    "    :param move: move: tuple representing a move from (peg1,peg2)\n",
    "    :return: the state after the move was made\n",
    "    '''\n",
    "    item = state[move[1]-1].pop(0)\n",
    "    state[move[0]-1].insert(0, item)\n",
    "    return state\n",
    "\n",
    "\n",
    "def winner(state):\n",
    "    '''\n",
    "    Determines if a winning state occured\n",
    "    :param state: list of lists representing tower of hanoi state\n",
    "    :return: True if winning state, False otherwise.\n",
    "    '''\n",
    "    board = [[], [], [1,2,3]]\n",
    "    return state == board\n",
    "\n",
    "\n",
    "def myTupler(state):\n",
    "    '''\n",
    "    Need immutable type for key to dictionary\n",
    "    :param state: list of lists representing tower of hanoi state\n",
    "    :return: tuple representation of the state\n",
    "    '''\n",
    "    superTuple = tuple(tuple(s) for s in state)\n",
    "    return superTuple\n",
    "\n",
    "\n",
    "def epsilonGreedy(epsilon, Q, state, validMovesF):\n",
    "    '''\n",
    "    Makes either a random move, or tries the move which Q indicates is the best.\n",
    "    :param epsilon: A decreasing number representing the level of randomness\n",
    "    :param Q: Dictionary of state,move - value pairs, with the higher values being better moves\n",
    "    :param state: list of lists representing tower of hanoi state\n",
    "    :param validMovesF: function returning valid moves\n",
    "    :return:\n",
    "    '''\n",
    "    goodMoves = validMovesF(state)\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Random Move\n",
    "        return tuple(random.choice(goodMoves))\n",
    "    else:\n",
    "        # Greedy Move\n",
    "        Qs = np.array([Q.get((myTupler(state),tuple(m)), 0.0) for m in goodMoves])\n",
    "        return tuple(goodMoves[np.argmax(Qs)])\n",
    "\n",
    "\n",
    "def trainQ(nRepetitions, learningRate, epsilonDecayFactor, validMovesF, makeMoveF):\n",
    "    '''\n",
    "    Creates and fills a dictionary, Q, representing the (state,move) - value pairs which, if followed\n",
    "    should create the shortest path to the solution.\n",
    "    :param nRepetitions: how many times to iterate through.  Higher numbers would generate more accurate results\n",
    "    :param learningRate: How much to adjust the value part of the dictionary\n",
    "    :param epsilonDecayFactor: how quickly to reduce the random factor.\n",
    "    :param validMovesF: function returning valid moves of a state\n",
    "    :param makeMoveF: function making a move on a state\n",
    "    :return: the dictionary, Q, and a list containing the number of steps it took per iteration to find the goal state\n",
    "    '''\n",
    "    maxGames = nRepetitions\n",
    "    rho = learningRate\n",
    "    epsilonDecayRate = epsilonDecayFactor\n",
    "    epsilon = 1.0\n",
    "    Q = {}\n",
    "    stepList = []\n",
    "    # show the moves while debuggin\n",
    "    showMoves = False\n",
    "\n",
    "    for nGames in range(maxGames):\n",
    "        # reduce the randomness every pass\n",
    "        epsilon *= epsilonDecayRate\n",
    "        step = 0\n",
    "        # hardcoded start state\n",
    "        state = [[1, 2, 3], [], []]\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            step += 1\n",
    "            # grab either a random or best of the known moves\n",
    "            move = epsilonGreedy(epsilon, Q, state, validMovesF)\n",
    "\n",
    "            # we don't want to change state directly, and because state is a list of lists, need to do a\n",
    "            # deepcopy on it, then make the move\n",
    "            stateNew = copy.deepcopy(state)\n",
    "            makeMoveF(stateNew, move)\n",
    "            \n",
    "            # if we haven't encountered this state,move combo, add it to Q\n",
    "            if (myTupler(state), move) not in Q:\n",
    "                Q[(myTupler(state), move)] = 0.0  # Initial Q value for new state, move\n",
    "            \n",
    "            # print if debugging\n",
    "            if showMoves:\n",
    "                printState(stateNew)\n",
    "            if winner(stateNew):\n",
    "                # We won!  backfill Q\n",
    "                if showMoves:\n",
    "                    print('End State, we won!')\n",
    "                Q[(myTupler(state), move)] = -1.0\n",
    "                done = True\n",
    "                # we're keeping a list of the number of steps it took for each winning solution, so add it here.\n",
    "                stepList.append(step)\n",
    "\n",
    "            # update the Q which led us here using the learning factor, and the difference between the current state\n",
    "            # and the old state\n",
    "            if step > 1:\n",
    "                Q[(myTupler(stateOld), moveOld)] += rho * (-1+Q[(myTupler(state), move)] - Q[(myTupler(stateOld), moveOld)])\n",
    "\n",
    "            # Store the current state, move so we can access it for the next Q update\n",
    "            stateOld, moveOld = state, move\n",
    "            state = stateNew\n",
    "\n",
    "    return Q,stepList\n",
    "\n",
    "\n",
    "def testQ(Q, maxSteps, validMovesF, makeMoveF):\n",
    "    '''\n",
    "    Using the dictionary Q, and the initial state of the game, traverse and return the best path.\n",
    "    :param Q: dictionary representing the (state,move) - value pairs which, if followed should create the shortest path to the solution.\n",
    "    :param maxSteps: The number of steps to attempt before giving up.\n",
    "    :param validMovesF: function returning valid moves of a state\n",
    "    :param makeMoveF: function making a move on a state\n",
    "    :return: list containing the states from start to finish\n",
    "    '''\n",
    "    state = [[1, 2, 3], [], []]\n",
    "    statePath = []\n",
    "    statePath.append(state)\n",
    "\n",
    "    for i in range(maxSteps):\n",
    "        if winner(state):\n",
    "            return statePath\n",
    "        goodMoves = validMovesF(state)\n",
    "        Qs = np.array([Q.get((myTupler(state), tuple(m)), 0.0) for m in goodMoves])\n",
    "        move = goodMoves[np.argmax(Qs)]\n",
    "        nextState = copy.deepcopy(state)\n",
    "        makeMoveF(nextState, move)\n",
    "        statePath.append(nextState)\n",
    "        state = nextState\n",
    "\n",
    "    return \"No path found\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing validMoves([[1], [2], [3]])\n",
      "\n",
      "--- 10/10 points. Correctly returned [[1, 2], [1, 3], [2, 3]]\n",
      "\n",
      "Testing validMoves([[], [], [1, 2, 3]])\n",
      "\n",
      "--- 10/10 points. Correctly returned [[3, 1], [3, 2]]\n",
      "\n",
      "Testing makeMove([[], [], [1, 2, 3]], [3, 2])\n",
      "\n",
      "--- 10/10 points. Correctly returned [[], [1], [2, 3]]\n",
      "\n",
      "Testing makeMove([[2], [3], [1]], [1, 2])\n",
      "\n",
      "--- 10/10 points. Correctly returned [[], [2, 3], [1]]\n",
      "\n",
      "Testing   Q, steps = trainQ(1000, 0.5, 0.7, validMoves, makeMove).\n",
      "\n",
      "--- 10/10 points. Q dictionary has correct number of entries.\n",
      "\n",
      "--- 10/10 points. The mean of the number of steps is 7.59 which is correct.\n",
      "\n",
      "Testing   path = testQ(Q, 20, validMoves, makeMove).\n",
      "\n",
      "--- 20/20 points. Correctly returns path of length 8, less than 10.\n",
      "\n",
      "A5 Execution Grade is 80/80\n",
      "\n",
      " Remaining 20 points will be based on your text describing the trainQ and test! functions.\n",
      "\n",
      "A5 FINAL GRADE is __/100\n"
     ]
    }
   ],
   "source": [
    "%run -i A5grader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
